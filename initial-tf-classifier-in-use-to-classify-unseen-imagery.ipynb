{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's the continuation. We use the TF Lite model to classify unseen pics to test how our model performs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import PIL\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Right, so below we need to set parameters same as in the training file, see ober-pipe-clasifier-initial-training.ipynb\n",
    "\n",
    "This below cell just uses the same params as what was used in training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_width, img_height = 300, 300\n",
    "\n",
    "class_names = ['damage_fracture_collapse', 'deposit_rubble_rubbish', 'joint_displacement', 'ok_condition', 'roots', 'water_present']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's bring out our TF lite model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "TF_MODEL_FILE_PATH = 'initial-model.tflite'\n",
    "\n",
    "interpreter = tf.lite.Interpreter(model_path=TF_MODEL_FILE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'serving_default': {'inputs': ['sequential_1_input'], 'outputs': ['outputs']}}"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "interpreter.get_signature_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.lite.python.interpreter.SignatureRunner at 0x21876ded690>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classify_lite = interpreter.get_signature_runner('serving_default')\n",
    "classify_lite"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A quick function, which opens the image for the model to look at"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_img(img_path, img_height, img_width):\n",
    "    img = tf.keras.utils.load_img(\n",
    "        img_path, target_size=(img_height, img_width)\n",
    "    )\n",
    "    img_array = tf.keras.utils.img_to_array(img)\n",
    "    img_array = tf.expand_dims(img_array, 0)\n",
    "    return img_array "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What happens here\n",
    "\n",
    "We basically have a look at a picture. We return a list with the 'class' first and then the score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def predict_and_classify_image(img):\n",
    "    img_array = open_img(img, img_height, img_width)\n",
    "    predictions_lite = classify_lite(sequential_1_input=img_array)['outputs']\n",
    "    score_lite = tf.nn.softmax(predictions_lite)\n",
    "\n",
    "    # print(\n",
    "    #     \"{} image most likely belongs to {} with a {:.2f} percent confidence.\"\n",
    "    #     .format(img, class_names[np.argmax(score_lite)], 100 * np.max(score_lite))\n",
    "    # )\n",
    "    return class_names[np.argmax(score_lite)], 100 * np.max(score_lite)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We look at all these 5000 images now from the 'unsorted' input folder. It contains randomly exported frames from hours of CCTV footage.\n",
    "\n",
    "We run the lite model on these and spit it each pic into the correct corresponding class (or at least this is our hope). if the pic shows fractured pipes it will be copied to\n",
    "\"./post-initial-training-classifier-output/damage_fracture_collapse\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_directory = './unsorted-images-for-post-initial-classification'\n",
    "output_directory = './post-initial-training-classifier-output'\n",
    "for image in os.listdir(input_directory):\n",
    "    image_to_classify = os.path.join(input_directory, image)\n",
    "    prediction = predict_and_classify_image(image_to_classify)\n",
    "    target_path = os.path.join(output_directory, prediction[0],image)\n",
    "    shutil.copy(image_to_classify, target_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How did the model do? OK for what was expected. We will need to manually look at the outputs and train a new network, which combines the original training batch and the tidied up and (semi-) manually labelled outpus to train a bigger model of improved accuracy. We might add some new classes, too, as obviously the model gets really confused when it looks at our starting manhole, for example."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
